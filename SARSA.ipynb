{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5zrrq1OW9uT",
        "outputId": "c67fb574-f84e-42f9-d352-8651ba5dcf6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-21d8c3886ff4>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward 0\n",
            "Episode 100, Total Reward -20000.0\n",
            "Episode 200, Total Reward -40000.0\n",
            "Episode 300, Total Reward -60000.0\n",
            "Episode 400, Total Reward -80000.0\n",
            "Episode 500, Total Reward -100000.0\n",
            "Episode 600, Total Reward -120000.0\n",
            "Episode 700, Total Reward -140000.0\n",
            "Episode 800, Total Reward -160000.0\n",
            "Episode 900, Total Reward -179936.0\n",
            "Episode 1000, Total Reward -199874.0\n",
            "Episode 1100, Total Reward -219628.0\n",
            "Episode 1200, Total Reward -238559.0\n",
            "Episode 1300, Total Reward -257777.0\n",
            "Episode 1400, Total Reward -276326.0\n",
            "Episode 1500, Total Reward -294332.0\n",
            "Episode 1600, Total Reward -314095.0\n",
            "Episode 1700, Total Reward -333883.0\n",
            "Episode 1800, Total Reward -353002.0\n",
            "Episode 1900, Total Reward -371373.0\n",
            "Episode 2000, Total Reward -388745.0\n",
            "Episode 2100, Total Reward -407438.0\n",
            "Episode 2200, Total Reward -426646.0\n",
            "Episode 2300, Total Reward -444533.0\n",
            "Episode 2400, Total Reward -461412.0\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 2500\n",
        "SHOW_EVERY = 100\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  # not a constant, going to be decayed\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES // 2\n",
        "epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "total_reward = 0  # Variable to store the total reward for each episode\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "    episode_reward = 0  # Initialize the episode reward to zero\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(f\"Episode {episode}, Total Reward {total_reward}\")\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    # Choose action using epsilon-greedy policy\n",
        "    action = np.argmax(q_table[discrete_state]) if np.random.random() > epsilon else np.random.randint(0, env.action_space.n)\n",
        "\n",
        "    while not done:\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward  # Accumulate the reward for each step\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "\n",
        "        # Choose next action using epsilon-greedy policy\n",
        "        next_action = np.argmax(q_table[new_discrete_state]) if np.random.random() > epsilon else np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        # Update Q-value using SARSA update rule\n",
        "        current_q = q_table[discrete_state + (action,)]\n",
        "        next_q = q_table[new_discrete_state + (next_action,)]\n",
        "        new_q = current_q + LEARNING_RATE * (reward + DISCOUNT * next_q - current_q)\n",
        "        q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "        action = next_action\n",
        "\n",
        "    total_reward += episode_reward  # Add the episode reward to the total reward\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "        epsilon -= epsilon_decay_value\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbWzGA6fYttn",
        "outputId": "8b42a1df-6ac5-475c-ef59-545acc5d3eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-1.79023553e+00 -2.63200727e-01 -2.26075042e-01]\n",
            "  [-9.92056427e-01 -1.93049391e+00 -1.39927523e+00]\n",
            "  [-1.45153166e+01 -1.45782390e+01 -1.43685590e+01]\n",
            "  ...\n",
            "  [-1.08961471e+00 -7.12270694e-01 -1.23964780e+00]\n",
            "  [-3.92735203e-01 -2.28641920e-02 -2.76197621e-01]\n",
            "  [-8.53430660e-01 -1.57872010e+00 -7.53769633e-01]]\n",
            "\n",
            " [[-1.10832119e+00 -1.67320397e+00 -1.91294738e+00]\n",
            "  [-6.63893989e+00 -6.30287324e+00 -6.10189696e+00]\n",
            "  [-1.41670279e+01 -1.42733567e+01 -1.42889844e+01]\n",
            "  ...\n",
            "  [-1.41315442e+00 -1.86849934e+00 -2.92183000e-01]\n",
            "  [-1.02457439e+00 -1.54569550e+00 -1.01094059e+00]\n",
            "  [-1.35712736e+00 -5.37574525e-01 -3.52467070e-01]]\n",
            "\n",
            " [[-2.34053438e-01 -7.39302679e-01 -8.23071422e-01]\n",
            "  [-9.98686421e+00 -1.04616794e+01 -1.04601871e+01]\n",
            "  [-1.40549372e+01 -1.40738238e+01 -1.42524862e+01]\n",
            "  ...\n",
            "  [-1.65683197e-01 -5.95759243e-01 -5.99871868e-01]\n",
            "  [-7.16426716e-01 -1.71989065e+00 -4.89258594e-01]\n",
            "  [-2.41028839e-01 -9.05894769e-02 -9.20955869e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-1.91994604e+00 -1.05093052e+00 -1.97399604e+00]\n",
            "  [-8.21167333e-01 -4.15136209e-01 -1.03340754e+00]\n",
            "  [-1.71267988e+00 -1.18579926e+00 -1.15448115e+00]\n",
            "  ...\n",
            "  [-2.19487455e-01 -1.54929288e+00 -1.66496504e+00]\n",
            "  [-1.54005016e+00 -7.96513819e-01 -1.63802459e+00]\n",
            "  [-1.31649672e+00 -6.83450439e-01 -5.82252164e-01]]\n",
            "\n",
            " [[-2.19667709e-01 -1.49989405e+00 -6.93428838e-01]\n",
            "  [-1.29073315e+00 -2.22582583e-01 -1.04669784e+00]\n",
            "  [-6.50934630e-01 -1.08252459e+00 -1.35350262e+00]\n",
            "  ...\n",
            "  [-1.39108807e+00 -1.80489639e+00 -1.89597167e-01]\n",
            "  [-1.31193211e+00 -1.78464637e+00 -7.23870316e-01]\n",
            "  [-9.56390767e-01 -1.49609860e+00 -5.01540683e-01]]\n",
            "\n",
            " [[-1.48789633e+00 -8.67923938e-01 -8.53206212e-01]\n",
            "  [-1.78546175e+00 -1.25432940e+00 -1.49808157e+00]\n",
            "  [-1.75447319e-03 -6.46403814e-01 -8.07590080e-01]\n",
            "  ...\n",
            "  [-6.62702991e-01 -1.04137987e+00 -1.02164453e+00]\n",
            "  [-1.14457867e+00 -9.09555375e-02 -1.53940482e+00]\n",
            "  [-2.84364772e-01 -1.52412371e+00 -9.47603356e-01]]]\n"
          ]
        }
      ]
    }
  ]
}