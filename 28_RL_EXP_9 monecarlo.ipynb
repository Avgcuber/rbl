{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "pipkoG7f-nfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size\n",
        "        self.grid = np.zeros((size, size))\n",
        "        self.grid[size-1, size-1] = 1  # Terminal state\n",
        "        self.current_position = (0, 0)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_position = (0, 0)\n",
        "        return self.current_position\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:  # Up\n",
        "            self.current_position = (max(self.current_position[0] - 1, 0), self.current_position[1])\n",
        "        elif action == 1:  # Down\n",
        "            self.current_position = (min(self.current_position[0] + 1, self.size - 1), self.current_position[1])\n",
        "        elif action == 2:  # Left\n",
        "            self.current_position = (self.current_position[0], max(self.current_position[1] - 1, 0))\n",
        "        elif action == 3:  # Right\n",
        "            self.current_position = (self.current_position[0], min(self.current_position[1] + 1, self.size - 1))\n",
        "\n",
        "        reward = 0\n",
        "        if self.current_position == (self.size - 1, self.size - 1):  # Reached terminal state\n",
        "            reward = 1\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        return self.current_position, reward, done, None"
      ],
      "metadata": {
        "id": "LXuCVK_I-pmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloAgent:\n",
        "    def __init__(self, num_states, num_actions, gamma=0.9):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.gamma = gamma\n",
        "        self.Q = np.zeros((num_states, num_actions))\n",
        "        self.returns_sum = np.zeros((num_states, num_actions))\n",
        "        self.returns_count = np.ones((num_states, num_actions))  # Initialized to 1 to avoid division by zero\n",
        "\n",
        "    def update_q_value(self, episode):\n",
        "        G = 0\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:  # First visit\n",
        "                self.returns_sum[state, action] += G\n",
        "                self.returns_count[state, action] += 1\n",
        "                self.Q[state, action] = self.returns_sum[state, action] / self.returns_count[state, action]\n",
        "\n",
        "    def get_action(self, state, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n"
      ],
      "metadata": {
        "id": "I2lwi30P-voH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TDAgent:\n",
        "    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.9):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        old_value = self.Q[state, action]\n",
        "        next_max = np.max(self.Q[next_state])\n",
        "        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)\n",
        "        self.Q[state, action] = new_value\n",
        "\n",
        "    def get_action(self, state, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])"
      ],
      "metadata": {
        "id": "P31azkz--zsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the agents\n",
        "num_episodes = 100\n",
        "epsilon = 0.1\n",
        "max_steps_per_episode = 100  # Maximum number of steps per episode\n",
        "\n",
        "env = GridWorld()\n",
        "num_states = env.size ** 2\n",
        "num_actions = 4"
      ],
      "metadata": {
        "id": "kmz0gQBy-2Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_mc = MonteCarloAgent(num_states, num_actions)\n",
        "agent_td = TDAgent(num_states, num_actions)"
      ],
      "metadata": {
        "id": "HmlqF72Y-5nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # Run episode with Monte Carlo agent\n",
        "    episode_mc = []  # Store (state, action, reward) tuples\n",
        "    while not done:\n",
        "        action = agent_mc.get_action(state[0] * env.size + state[1], epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_mc.append((state[0] * env.size + state[1], action, reward))  # Flatten state\n",
        "        state = next_state\n",
        "    agent_mc.update_q_value(episode_mc)\n"
      ],
      "metadata": {
        "id": "N-8j9r7q_A62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "# Run episode with Temporal-Difference agent\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent_td.get_action(state[0] * env.size + state[1], epsilon)  # Flatten state\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent_td.update_q_value(state[0] * env.size + state[1], action, reward, next_state[0] * env.size + next_state[1])\n",
        "        state = next_state"
      ],
      "metadata": {
        "id": "SxSsEV0V_H37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_agent(agent, env, num_episodes=100):\n",
        "    total_rewards = []\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            action = agent.get_action(state[0] * env.size + state[1], epsilon=0)  # Greedy action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "        total_rewards.append(episode_reward)\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    return avg_reward\n",
        "\n",
        "# Evaluate agents\n",
        "avg_reward_mc = evaluate_agent(agent_mc, env)\n",
        "avg_reward_td = evaluate_agent(agent_td, env)\n",
        "\n",
        "print(\"Average reward for Monte Carlo agent:\", avg_reward_mc)\n",
        "print(\"Average reward for Temporal-Difference agent:\", avg_reward_td)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcpu3x7i_XH_",
        "outputId": "a194163b-eb1a-4350-951d-10d3c5b85edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward for Monte Carlo agent: 1.0\n",
            "Average reward for Temporal-Difference agent: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison of Policies\n",
        "print(\"Monte Carlo agent's Q-values:\")\n",
        "print(agent_mc.Q)\n",
        "print(\"Temporal-Difference agent's Q-values:\")\n",
        "print(agent_td.Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwd9brR-_uBF",
        "outputId": "5fc25517-7516-4cb4-8b39-3ae90954b415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo agent's Q-values:\n",
            "[[5.28797015e-003 3.56968197e-003 4.11969767e-003 1.90283322e-001]\n",
            " [5.24364339e-003 3.33626481e-001 1.67823474e-002 1.13356804e-002]\n",
            " [1.02260738e-003 1.71529412e-001 5.74198627e-005 5.83078385e-004]\n",
            " [1.14227977e-320 3.39097015e-319 5.89010835e-002 1.40072551e-319]\n",
            " [1.16535878e-002 0.00000000e+000 4.49819622e-006 9.01850075e-253]\n",
            " [7.38112500e-003 6.56100000e-002 1.19574225e-001 6.61013171e-001]\n",
            " [8.55782609e-002 3.07273500e-001 4.68642857e-001 7.73427706e-001]\n",
            " [8.43557143e-002 8.60419904e-001 2.35960327e-305 4.05000000e-001]\n",
            " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
            " [7.29000000e-002 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
            " [4.86000000e-001 1.96830000e-001 0.00000000e+000 0.00000000e+000]\n",
            " [3.07800000e-001 9.90291262e-001 4.92075000e-001 3.64500000e-001]\n",
            " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
            " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
            " [2.18700000e-001 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
            " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]]\n",
            "Temporal-Difference agent's Q-values:\n",
            "[[0.17770109 0.0458963  0.242952   0.59047601]\n",
            " [0.22783193 0.01505867 0.07197416 0.65609677]\n",
            " [0.32317034 0.07642042 0.25916114 0.72899934]\n",
            " [0.32905615 0.8099999  0.02930357 0.32661048]\n",
            " [0.22363273 0.         0.         0.        ]\n",
            " [0.16049023 0.         0.         0.        ]\n",
            " [0.38913216 0.         0.         0.15389429]\n",
            " [0.2333285  0.89999999 0.05809612 0.26210719]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.06230217 0.         0.         0.        ]\n",
            " [0.21308423 1.         0.0063617  0.4662342 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison of State Values\n",
        "print(\"Monte Carlo agent's learned state values:\")\n",
        "print(np.max(agent_mc.Q, axis=1))\n",
        "print(\"Temporal-Difference agent's learned state values:\")\n",
        "print(np.max(agent_td.Q, axis=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrnSZw00_wZo",
        "outputId": "180b91a8-afdd-4970-8280-d5a878fb6196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo agent's learned state values:\n",
            "[0.19028332 0.33362648 0.17152941 0.05890108 0.01165359 0.66101317\n",
            " 0.77342771 0.8604199  0.         0.0729     0.486      0.99029126\n",
            " 0.         0.         0.2187     0.        ]\n",
            "Temporal-Difference agent's learned state values:\n",
            "[0.59047601 0.65609677 0.72899934 0.8099999  0.22363273 0.16049023\n",
            " 0.38913216 0.89999999 0.         0.         0.06230217 1.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    }
  ]
}