{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOPP5olpEtVa",
        "outputId": "3dbed535-d6d1-4f1d-a9d6-7b85dfcbe827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.48.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 2500\n",
        "SHOW_EVERY = 100\n",
        "\n",
        "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  # not a constant, going to be decayed\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODES // 2\n",
        "epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
        "\n",
        "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
        "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
        "\n",
        "total_reward = 0  # Variable to store the total reward for each episode\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    discrete_state = get_discrete_state(env.reset())\n",
        "    done = False\n",
        "    episode_reward = 0  # Initialize the episode reward to zero\n",
        "\n",
        "    if episode % SHOW_EVERY == 0:\n",
        "        render = True\n",
        "        print(f\"Episode {episode}, Total Reward {total_reward}\")\n",
        "    else:\n",
        "        render = False\n",
        "\n",
        "    while not done:\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table\n",
        "            action = np.argmax(q_table[discrete_state])\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward  # Accumulate the reward for each step\n",
        "\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % SHOW_EVERY == 0:\n",
        "            env.render()\n",
        "\n",
        "        if not done:\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "        elif new_state[0] >= env.goal_position:\n",
        "            # Take the goal reward directly from the environment\n",
        "            q_table[discrete_state + (action,)] = reward\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    total_reward += episode_reward  # Add the episode reward to the total reward\n",
        "\n",
        "    # Decaying is being done every episode if episode number is within decaying range\n",
        "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "        epsilon -= epsilon_decay_value\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg4lZSotWJNq",
        "outputId": "be5bb34c-57b5-4ecf-f92a-17e146695cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-074195718c6d>:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward 0\n",
            "Episode 100, Total Reward -20000.0\n",
            "Episode 200, Total Reward -40000.0\n",
            "Episode 300, Total Reward -60000.0\n",
            "Episode 400, Total Reward -80000.0\n",
            "Episode 500, Total Reward -100000.0\n",
            "Episode 600, Total Reward -120000.0\n",
            "Episode 700, Total Reward -140000.0\n",
            "Episode 800, Total Reward -160000.0\n",
            "Episode 900, Total Reward -179972.0\n",
            "Episode 1000, Total Reward -199972.0\n",
            "Episode 1100, Total Reward -219966.0\n",
            "Episode 1200, Total Reward -239707.0\n",
            "Episode 1300, Total Reward -259214.0\n",
            "Episode 1400, Total Reward -277533.0\n",
            "Episode 1500, Total Reward -296649.0\n",
            "Episode 1600, Total Reward -316564.0\n",
            "Episode 1700, Total Reward -336275.0\n",
            "Episode 1800, Total Reward -355583.0\n",
            "Episode 1900, Total Reward -374860.0\n",
            "Episode 2000, Total Reward -394137.0\n",
            "Episode 2100, Total Reward -413171.0\n",
            "Episode 2200, Total Reward -432037.0\n",
            "Episode 2300, Total Reward -449219.0\n",
            "Episode 2400, Total Reward -467178.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efgvFenyWZME",
        "outputId": "da2c8d50-ae8b-42ba-b7b3-0db7202202ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-1.86453443e+00 -1.20199128e-01 -1.71365996e+00]\n",
            "  [-1.08163019e+00 -3.85440654e-01 -7.11909801e-02]\n",
            "  [-1.27468243e+01 -1.28780235e+01 -1.26667057e+01]\n",
            "  ...\n",
            "  [-6.72229030e-01 -1.42695717e+00 -6.72530775e-01]\n",
            "  [-1.19145154e+00 -1.26830679e+00 -1.67767871e+00]\n",
            "  [-1.82502446e+00 -1.87941135e+00 -1.51429879e+00]]\n",
            "\n",
            " [[-1.36748677e+00 -8.95473305e-01 -1.37185111e+00]\n",
            "  [-5.60634602e+00 -5.26379366e+00 -5.47159423e+00]\n",
            "  [-1.21985972e+01 -1.18176018e+01 -1.20296225e+01]\n",
            "  ...\n",
            "  [-7.24310457e-01 -1.90952085e+00 -6.60787796e-01]\n",
            "  [-5.81838832e-01 -1.14494814e+00 -7.35246804e-01]\n",
            "  [-5.09658313e-01 -6.88854538e-01 -5.69446030e-01]]\n",
            "\n",
            " [[-1.24391783e+00 -2.32475138e-01 -1.47074683e+00]\n",
            "  [-6.16388867e+00 -6.07084297e+00 -5.69219857e+00]\n",
            "  [-1.20075513e+01 -1.27431026e+01 -1.29177017e+01]\n",
            "  ...\n",
            "  [-3.20530831e-01 -1.33794092e+00 -7.96830244e-01]\n",
            "  [-3.94104126e-01 -9.47768606e-01 -1.53602281e+00]\n",
            "  [-1.15430249e+00 -7.55562327e-01 -7.15514340e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.28450787e-01 -1.39055702e+00 -6.35046235e-02]\n",
            "  [-5.52901425e-01 -2.06824017e-01 -7.96404300e-01]\n",
            "  [-3.14064113e-01 -9.78109682e-01 -5.38910710e-01]\n",
            "  ...\n",
            "  [-3.94102049e-01 -1.67539844e-01 -7.95714693e-02]\n",
            "  [-1.70468946e+00 -1.43463621e+00 -1.02600637e+00]\n",
            "  [-1.02425105e+00 -9.22520768e-01 -1.80714668e+00]]\n",
            "\n",
            " [[-1.17266748e-01 -1.67483353e+00 -9.98639278e-01]\n",
            "  [-1.15896190e+00 -1.15532551e+00 -4.20227328e-01]\n",
            "  [-7.54280475e-01 -4.37851846e-02 -2.49945630e-01]\n",
            "  ...\n",
            "  [-6.94956376e-01 -1.98873612e+00 -1.13411424e-01]\n",
            "  [-1.24791230e+00 -1.89152399e+00 -1.54222181e-01]\n",
            "  [-4.91878910e-01 -1.59254275e-01 -1.08647640e+00]]\n",
            "\n",
            " [[-2.40244407e-02 -1.85487810e+00 -1.30929530e+00]\n",
            "  [-1.17129426e+00 -1.66196541e+00 -9.33020722e-01]\n",
            "  [-1.98256515e-01 -1.98722978e+00 -1.00312301e-01]\n",
            "  ...\n",
            "  [-1.23497009e+00 -1.40015522e+00 -6.56276549e-01]\n",
            "  [-1.85179319e+00 -7.75272968e-02 -4.33214984e-01]\n",
            "  [-1.22720918e+00 -1.55372003e-01 -7.64460842e-01]]]\n"
          ]
        }
      ]
    }
  ]
}